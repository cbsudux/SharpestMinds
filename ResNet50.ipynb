{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os, cv2\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.applications import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:55<00:00,  4.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4750, 256, 256, 3)\n",
      "(4750, 12)\n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "label_map = {   \"Black-grass\"               :0,\n",
    "                \"Charlock\"                  :1,\n",
    "                \"Cleavers\"                  :2,\n",
    "                \"Common Chickweed\"          :3,\n",
    "                \"Common wheat\"              :4,\n",
    "                \"Fat Hen\"                   :5,\n",
    "                \"Loose Silky-bent\"          :6,\n",
    "                \"Maize\"                     :7,\n",
    "                \"Scentless Mayweed\"         :8,\n",
    "                \"Shepherds Purse\"           :9,\n",
    "                \"Small-flowered Cranesbill\" :10,\n",
    "                \"Sugar beet\"                :11}\n",
    "\n",
    "dim = 256\n",
    "\n",
    "# Preparing training data\n",
    "dirs = os.listdir(\"/input/plant_seedlings_classification/train/\")\n",
    "for k in tqdm(range(len(dirs))):    # Directory\n",
    "    files = os.listdir(\"/input/plant_seedlings_classification/train/{}\".format(dirs[k]))\n",
    "    for f in range(len(files)):     # Files\n",
    "        img = cv2.imread('/input/plant_seedlings_classification/train/{}/{}'.format(dirs[k], files[f]))\n",
    "        targets = np.zeros(12)\n",
    "        targets[label_map[dirs[k]]] = 1 \n",
    "        x_train.append(cv2.resize(img, (dim, dim)))\n",
    "        y_train.append(targets)\n",
    "    \n",
    "y_train = np.array(y_train, np.uint8)\n",
    "x_train = np.array(x_train, np.float32)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "#x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.01, random_state=42)\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.16, random_state=42) # Want a balanced split for all the classes\n",
    "for train_index, test_index in sss.split(x_train, y_train):\n",
    "    print(\"Using {} for training and {} for validation\".format(len(train_index), len(test_index)))\n",
    "    x_train, x_valid = x_train[train_index], x_train[test_index]\n",
    "    y_train, y_valid = y_train[train_index], y_train[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 14s 0us/step\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator( horizontal_flip=True, \n",
    "                              vertical_flip=True,\n",
    "                            rescale = 1./255)\n",
    "                                      \n",
    "weights = os.path.join('', '/output/weights.h5')\n",
    "\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "\n",
    "callbacks = [ EarlyStopping(monitor='val_loss', patience=5, verbose=0), \n",
    "              ModelCheckpoint(weights, monitor='val_loss', save_best_only=True, verbose=0),\n",
    "              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)]\n",
    "\n",
    "base_model = ResNet50(input_shape=(dim, dim, 3), include_top=False, weights='imagenet', pooling='avg') # Average pooling reduces output dimensions\n",
    "x = base_model.output\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(12, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Train FCN on top with layers frozen\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=learning_rate), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/21\n",
      "125/124 [==============================] - 68s 545ms/step - loss: 0.4274 - acc: 0.8477 - val_loss: 3.5643 - val_acc: 0.1868\n",
      "Epoch 2/21\n",
      "125/124 [==============================] - 69s 548ms/step - loss: 0.4254 - acc: 0.8620 - val_loss: 2.1988 - val_acc: 0.4316\n",
      "Epoch 3/21\n",
      "125/124 [==============================] - 69s 548ms/step - loss: 0.4063 - acc: 0.8649 - val_loss: 0.9302 - val_acc: 0.6868\n",
      "Epoch 4/21\n",
      "125/124 [==============================] - 68s 547ms/step - loss: 0.4095 - acc: 0.8573 - val_loss: 0.4217 - val_acc: 0.8526\n",
      "Epoch 5/21\n",
      "125/124 [==============================] - 68s 547ms/step - loss: 0.4125 - acc: 0.8581 - val_loss: 0.3601 - val_acc: 0.8697\n",
      "Epoch 6/21\n",
      "125/124 [==============================] - 69s 548ms/step - loss: 0.4162 - acc: 0.8594 - val_loss: 0.3540 - val_acc: 0.8711\n",
      "Epoch 7/21\n",
      "125/124 [==============================] - 68s 548ms/step - loss: 0.4000 - acc: 0.8673 - val_loss: 0.3344 - val_acc: 0.8842\n",
      "Epoch 8/21\n",
      "125/124 [==============================] - 68s 544ms/step - loss: 0.4053 - acc: 0.8581 - val_loss: 0.3832 - val_acc: 0.8645\n",
      "Epoch 9/21\n",
      "125/124 [==============================] - 68s 544ms/step - loss: 0.4096 - acc: 0.8598 - val_loss: 0.3403 - val_acc: 0.8855\n",
      "Epoch 10/21\n",
      "125/124 [==============================] - 68s 545ms/step - loss: 0.3961 - acc: 0.8645 - val_loss: 0.3600 - val_acc: 0.8697\n",
      "Epoch 11/21\n",
      "125/124 [==============================] - 68s 545ms/step - loss: 0.4077 - acc: 0.8597 - val_loss: 0.3451 - val_acc: 0.8776\n",
      "Epoch 12/21\n",
      "125/124 [==============================] - 68s 544ms/step - loss: 0.4167 - acc: 0.8617 - val_loss: 0.3697 - val_acc: 0.8684\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2cb6e67e48>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=len(x_train)/batch_size, \n",
    "                    validation_data=datagen.flow(x_valid, y_valid, batch_size=batch_size), \n",
    "                    validation_steps=len(x_valid)/batch_size,\n",
    "                    callbacks=callbacks,\n",
    "                    epochs=21, \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune \n",
    "# load best weights \n",
    "\n",
    "model.load_weights('/output/weights.h5')\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "weights = os.path.join('', '/output/finetune_weights.h5')\n",
    "\n",
    "callbacks = [ EarlyStopping(monitor='val_loss', patience=5, verbose=0), \n",
    "              ModelCheckpoint(weights, monitor='val_loss', save_best_only=True, verbose=0),\n",
    "              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)]\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=learning_rate), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "125/124 [==============================] - 195s 2s/step - loss: 1.2540 - acc: 0.6630 - val_loss: 4.6617 - val_acc: 0.3697\n",
      "Epoch 2/30\n",
      "125/124 [==============================] - 189s 2s/step - loss: 0.4798 - acc: 0.8354 - val_loss: 0.5160 - val_acc: 0.8329\n",
      "Epoch 3/30\n",
      "125/124 [==============================] - 187s 1s/step - loss: 0.4232 - acc: 0.8592 - val_loss: 0.7804 - val_acc: 0.7368\n",
      "Epoch 4/30\n",
      "125/124 [==============================] - 188s 2s/step - loss: 0.3040 - acc: 0.8964 - val_loss: 0.4777 - val_acc: 0.8579\n",
      "Epoch 5/30\n",
      "125/124 [==============================] - 187s 1s/step - loss: 0.2780 - acc: 0.9039 - val_loss: 4.2932 - val_acc: 0.4487\n",
      "Epoch 6/30\n",
      "125/124 [==============================] - 187s 1s/step - loss: 0.2738 - acc: 0.9000 - val_loss: 0.5582 - val_acc: 0.8250\n",
      "Epoch 7/30\n",
      "125/124 [==============================] - 188s 2s/step - loss: 0.2327 - acc: 0.9202 - val_loss: 2.0754 - val_acc: 0.5855\n",
      "Epoch 8/30\n",
      "125/124 [==============================] - 188s 2s/step - loss: 0.1539 - acc: 0.9484 - val_loss: 0.1702 - val_acc: 0.9408\n",
      "Epoch 9/30\n",
      "125/124 [==============================] - 188s 2s/step - loss: 0.1023 - acc: 0.9629 - val_loss: 0.1297 - val_acc: 0.9539\n",
      "Epoch 10/30\n",
      "125/124 [==============================] - 187s 1s/step - loss: 0.0919 - acc: 0.9700 - val_loss: 0.1530 - val_acc: 0.9434\n",
      "Epoch 11/30\n",
      "125/124 [==============================] - 188s 2s/step - loss: 0.0788 - acc: 0.9747 - val_loss: 0.1248 - val_acc: 0.9500\n",
      "Epoch 12/30\n",
      "125/124 [==============================] - 187s 1s/step - loss: 0.0655 - acc: 0.9779 - val_loss: 0.1304 - val_acc: 0.9579\n",
      "Epoch 13/30\n",
      "125/124 [==============================] - 188s 2s/step - loss: 0.0625 - acc: 0.9822 - val_loss: 0.1142 - val_acc: 0.9553\n",
      "Epoch 14/30\n",
      "125/124 [==============================] - 187s 1s/step - loss: 0.0516 - acc: 0.9840 - val_loss: 0.1318 - val_acc: 0.9500\n",
      "Epoch 15/30\n",
      "125/124 [==============================] - 189s 2s/step - loss: 0.0548 - acc: 0.9814 - val_loss: 0.1079 - val_acc: 0.9658\n",
      "Epoch 16/30\n",
      "125/124 [==============================] - 187s 1s/step - loss: 0.0516 - acc: 0.9844 - val_loss: 0.1236 - val_acc: 0.9553\n",
      "Epoch 17/30\n",
      "125/124 [==============================] - 188s 2s/step - loss: 0.0482 - acc: 0.9857 - val_loss: 0.1030 - val_acc: 0.9632\n",
      "Epoch 18/30\n",
      "125/124 [==============================] - 187s 1s/step - loss: 0.0350 - acc: 0.9904 - val_loss: 0.1505 - val_acc: 0.9553\n",
      "Epoch 19/30\n",
      "125/124 [==============================] - 187s 1s/step - loss: 0.0397 - acc: 0.9867 - val_loss: 0.1413 - val_acc: 0.9579\n",
      "Epoch 20/30\n",
      "125/124 [==============================] - 187s 1s/step - loss: 0.0330 - acc: 0.9901 - val_loss: 0.1097 - val_acc: 0.9645\n",
      "Epoch 21/30\n",
      "125/124 [==============================] - 188s 2s/step - loss: 0.0260 - acc: 0.9917 - val_loss: 0.0947 - val_acc: 0.9658\n",
      "Epoch 22/30\n",
      "125/124 [==============================] - 187s 1s/step - loss: 0.0211 - acc: 0.9947 - val_loss: 0.1260 - val_acc: 0.9632\n",
      "Epoch 23/30\n",
      "125/124 [==============================] - 187s 1s/step - loss: 0.0213 - acc: 0.9930 - val_loss: 0.1215 - val_acc: 0.9658\n",
      "Epoch 24/30\n",
      "125/124 [==============================] - 187s 1s/step - loss: 0.0240 - acc: 0.9927 - val_loss: 0.1269 - val_acc: 0.9632\n",
      "Epoch 25/30\n",
      "125/124 [==============================] - 187s 1s/step - loss: 0.0163 - acc: 0.9965 - val_loss: 0.1076 - val_acc: 0.9645\n",
      "Epoch 26/30\n",
      "125/124 [==============================] - 188s 2s/step - loss: 0.0184 - acc: 0.9947 - val_loss: 0.1143 - val_acc: 0.9658\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2cb7474668>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=len(x_train)/batch_size, \n",
    "                    validation_data=datagen.flow(x_valid, y_valid, batch_size=batch_size), \n",
    "                    validation_steps=len(x_valid)/batch_size,\n",
    "                    callbacks=callbacks,\n",
    "                    epochs=30, \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 794/794 [00:22<00:00, 35.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(794, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "# ------ TESTING ------\n",
    "x_test = []\n",
    "df_test = pd.read_csv('/input/plant_seedlings_classification/sample_submission.csv')\n",
    "\n",
    "for f, species in tqdm(df_test.values, miniters=100):\n",
    "    img = cv2.imread('/input/plant_seedlings_classification/test/{}'.format(f))\n",
    "    x_test.append(cv2.resize(img, (dim, dim)))\n",
    "\n",
    "x_test = np.array(x_test, np.float32)\n",
    "print(x_test.shape)\n",
    "\n",
    "x_test = x_test /255.0\n",
    "\n",
    "if os.path.isfile(weights):\n",
    "    model.load_weights(weights)\n",
    "\n",
    "p_test = model.predict(x_test, verbose = 1)\n",
    "\n",
    "preds = []\n",
    "for i in range(len(p_test)):\n",
    "    pos = np.argmax(p_test[i])\n",
    "    preds.append(list(label_map.keys())[list(label_map.values()).index(pos)])\n",
    "\n",
    "df_test['species'] = preds\n",
    "df_test.to_csv('/output/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
