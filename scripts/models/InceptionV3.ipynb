{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os, cv2\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.applications import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [02:01<00:00,  8.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4750, 299, 299, 3)\n",
      "(4750, 12)\n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "label_map = {   \"Black-grass\"               :0,\n",
    "                \"Charlock\"                  :1,\n",
    "                \"Cleavers\"                  :2,\n",
    "                \"Common Chickweed\"          :3,\n",
    "                \"Common wheat\"              :4,\n",
    "                \"Fat Hen\"                   :5,\n",
    "                \"Loose Silky-bent\"          :6,\n",
    "                \"Maize\"                     :7,\n",
    "                \"Scentless Mayweed\"         :8,\n",
    "                \"Shepherds Purse\"           :9,\n",
    "                \"Small-flowered Cranesbill\" :10,\n",
    "                \"Sugar beet\"                :11}\n",
    "\n",
    "dim = 299\n",
    "\n",
    "# Preparing training data\n",
    "dirs = os.listdir(\"/input/plant_seedlings_classification/train/\")\n",
    "for k in tqdm(range(len(dirs))):    # Directory\n",
    "    files = os.listdir(\"/input/plant_seedlings_classification/train/{}\".format(dirs[k]))\n",
    "    for f in range(len(files)):     # Files\n",
    "        img = cv2.imread('/input/plant_seedlings_classification/train/{}/{}'.format(dirs[k], files[f]))\n",
    "        targets = np.zeros(12)\n",
    "        targets[label_map[dirs[k]]] = 1 \n",
    "        x_train.append(cv2.resize(img, (dim, dim)))\n",
    "        y_train.append(targets)\n",
    "    \n",
    "y_train = np.array(y_train, np.uint8)\n",
    "x_train = np.array(x_train, np.float32)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "#x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.01, random_state=42)\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.16, random_state=42) # Want a balanced split for all the classes\n",
    "for train_index, test_index in sss.split(x_train, y_train):\n",
    "    print(\"Using {} for training and {} for validation\".format(len(train_index), len(test_index)))\n",
    "    x_train, x_valid = x_train[train_index], x_train[test_index]\n",
    "    y_train, y_valid = y_train[train_index], y_train[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "85565440/87910968 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        rotation_range = 40,\n",
    "        horizontal_flip=True)\n",
    "                                  \n",
    "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "                                      \n",
    "weights = os.path.join('', '/output/weights.h5')\n",
    "\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "\n",
    "callbacks = [ EarlyStopping(monitor='val_loss', patience=5, verbose=0), \n",
    "              ModelCheckpoint(weights, monitor='val_loss', save_best_only=True, verbose=0),\n",
    "              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)]\n",
    "\n",
    "base_model = InceptionV3(input_shape=(dim, dim, 3), include_top=False, weights='imagenet', pooling='avg') # Average pooling reduces output dimensions\n",
    "x = base_model.output\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(12, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Training FCN on top\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=learning_rate), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "125/124 [==============================] - 233s - loss: 1.1383 - acc: 0.6498 - val_loss: 6.7509 - val_acc: 0.3237\n",
      "Epoch 2/30\n",
      "125/124 [==============================] - 207s - loss: 0.6516 - acc: 0.8020 - val_loss: 2.0757 - val_acc: 0.5382\n",
      "Epoch 3/30\n",
      "125/124 [==============================] - 207s - loss: 0.4453 - acc: 0.8476 - val_loss: 0.7841 - val_acc: 0.7487\n",
      "Epoch 4/30\n",
      "125/124 [==============================] - 206s - loss: 0.4157 - acc: 0.8683 - val_loss: 8.1770 - val_acc: 0.2737\n",
      "Epoch 5/30\n",
      "125/124 [==============================] - 206s - loss: 0.3836 - acc: 0.8762 - val_loss: 2.0852 - val_acc: 0.6434\n",
      "Epoch 6/30\n",
      "125/124 [==============================] - 207s - loss: 0.3076 - acc: 0.8912 - val_loss: 3.0793 - val_acc: 0.6303\n",
      "Epoch 7/30\n",
      "125/124 [==============================] - 207s - loss: 0.1847 - acc: 0.9442 - val_loss: 0.1500 - val_acc: 0.9553\n",
      "Epoch 8/30\n",
      "125/124 [==============================] - 207s - loss: 0.1404 - acc: 0.9539 - val_loss: 0.1044 - val_acc: 0.9684\n",
      "Epoch 9/30\n",
      "125/124 [==============================] - 206s - loss: 0.1278 - acc: 0.9590 - val_loss: 0.1408 - val_acc: 0.9605\n",
      "Epoch 10/30\n",
      "125/124 [==============================] - 206s - loss: 0.1026 - acc: 0.9670 - val_loss: 0.1226 - val_acc: 0.9658\n",
      "Epoch 11/30\n",
      "125/124 [==============================] - 206s - loss: 0.0985 - acc: 0.9680 - val_loss: 0.1338 - val_acc: 0.9592\n",
      "Epoch 12/30\n",
      "125/124 [==============================] - 207s - loss: 0.0964 - acc: 0.9680 - val_loss: 0.0981 - val_acc: 0.9697\n",
      "Epoch 13/30\n",
      "125/124 [==============================] - 206s - loss: 0.0924 - acc: 0.9695 - val_loss: 0.1130 - val_acc: 0.9618\n",
      "Epoch 14/30\n",
      "125/124 [==============================] - 206s - loss: 0.0816 - acc: 0.9727 - val_loss: 0.1069 - val_acc: 0.9724\n",
      "Epoch 15/30\n",
      "125/124 [==============================] - 207s - loss: 0.0817 - acc: 0.9730 - val_loss: 0.0895 - val_acc: 0.9724\n",
      "Epoch 16/30\n",
      "125/124 [==============================] - 207s - loss: 0.0826 - acc: 0.9725 - val_loss: 0.1159 - val_acc: 0.9658\n",
      "Epoch 17/30\n",
      "125/124 [==============================] - 206s - loss: 0.0766 - acc: 0.9739 - val_loss: 0.1018 - val_acc: 0.9684\n",
      "Epoch 18/30\n",
      "125/124 [==============================] - 206s - loss: 0.0895 - acc: 0.9723 - val_loss: 0.1042 - val_acc: 0.9697\n",
      "Epoch 19/30\n",
      "125/124 [==============================] - 207s - loss: 0.0662 - acc: 0.9794 - val_loss: 0.1023 - val_acc: 0.9697\n",
      "Epoch 20/30\n",
      "125/124 [==============================] - 207s - loss: 0.0696 - acc: 0.9771 - val_loss: 0.1040 - val_acc: 0.9671\n",
      "Epoch 21/30\n",
      "125/124 [==============================] - 207s - loss: 0.0741 - acc: 0.9784 - val_loss: 0.0946 - val_acc: 0.9711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f804d5c0160>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=len(x_train)/batch_size, \n",
    "                    validation_data=val_datagen.flow(x_valid, y_valid, batch_size=batch_size), \n",
    "                    validation_steps=len(x_valid)/batch_size,\n",
    "                    callbacks=callbacks,\n",
    "                    epochs=30, \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tuning \n",
    "\n",
    "model.load_weights(weights)\n",
    "\n",
    "for layer in model.layers[:172]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[172:]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "finetune_weights = os.path.join('', '/output/finetune_weights.h5')\n",
    "\n",
    "learning_rate = 0.0001\n",
    "batch_size = 32\n",
    "\n",
    "callbacks = [ EarlyStopping(monitor='val_loss', patience=5, verbose=0), \n",
    "              ModelCheckpoint(finetune_weights, monitor='val_loss', save_best_only=True, verbose=0),\n",
    "              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)]\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=learning_rate), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "124/124 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9773Epoch 00000: val_loss improved from inf to 0.09675, saving model to /output/finetune_weights.h5\n",
      "125/124 [==============================] - 116s - loss: 0.0775 - acc: 0.9775 - val_loss: 0.0967 - val_acc: 0.9724\n",
      "Epoch 2/50\n",
      "124/124 [============================>.] - ETA: 0s - loss: 0.0888 - acc: 0.9723Epoch 00001: val_loss did not improve\n",
      "125/124 [==============================] - 111s - loss: 0.0887 - acc: 0.9721 - val_loss: 0.1051 - val_acc: 0.9750\n",
      "Epoch 3/50\n",
      "124/124 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9758Epoch 00002: val_loss improved from 0.09675 to 0.08139, saving model to /output/finetune_weights.h5\n",
      "125/124 [==============================] - 112s - loss: 0.0713 - acc: 0.9760 - val_loss: 0.0814 - val_acc: 0.9724\n",
      "Epoch 4/50\n",
      "124/124 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9761Epoch 00003: val_loss did not improve\n",
      "125/124 [==============================] - 111s - loss: 0.0754 - acc: 0.9762 - val_loss: 0.1253 - val_acc: 0.9632\n",
      "Epoch 5/50\n",
      "124/124 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9756Epoch 00004: val_loss did not improve\n",
      "125/124 [==============================] - 111s - loss: 0.0769 - acc: 0.9754 - val_loss: 0.1062 - val_acc: 0.9697\n",
      "Epoch 6/50\n",
      "124/124 [============================>.] - ETA: 0s - loss: 0.0737 - acc: 0.9783Epoch 00005: val_loss did not improve\n",
      "\n",
      "Epoch 00005: reducing learning rate to 9.999999747378752e-06.\n",
      "125/124 [==============================] - 113s - loss: 0.0796 - acc: 0.9767 - val_loss: 0.1049 - val_acc: 0.9671\n",
      "Epoch 7/50\n",
      "124/124 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9725Epoch 00006: val_loss did not improve\n",
      "125/124 [==============================] - 111s - loss: 0.0797 - acc: 0.9727 - val_loss: 0.1313 - val_acc: 0.9605\n",
      "Epoch 8/50\n",
      "124/124 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9768Epoch 00007: val_loss did not improve\n",
      "\n",
      "Epoch 00007: reducing learning rate to 9.999999747378752e-07.\n",
      "125/124 [==============================] - 111s - loss: 0.0765 - acc: 0.9766 - val_loss: 0.1064 - val_acc: 0.9671\n",
      "Epoch 9/50\n",
      "124/124 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9761Epoch 00008: val_loss did not improve\n",
      "125/124 [==============================] - 111s - loss: 0.0762 - acc: 0.9762 - val_loss: 0.1242 - val_acc: 0.9645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7fd1f336a0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=len(x_train)/batch_size, \n",
    "                    validation_data=val_datagen.flow(x_valid, y_valid, batch_size=batch_size), \n",
    "                    validation_steps=len(x_valid)/batch_size,\n",
    "                    callbacks=callbacks,\n",
    "                    epochs=50, \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 794/794 [00:05<00:00, 143.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(794, 299, 299, 3)\n"
     ]
    }
   ],
   "source": [
    "# ------ TESTING ------\n",
    "x_test = []\n",
    "df_test = pd.read_csv('/input/plant_seedlings_classification/sample_submission.csv')\n",
    "\n",
    "for f, species in tqdm(df_test.values, miniters=100):\n",
    "    img = cv2.imread('/input/plant_seedlings_classification/test/{}'.format(f))\n",
    "    x_test.append(cv2.resize(img, (dim, dim)))\n",
    "\n",
    "x_test = np.array(x_test, np.float32)\n",
    "print(x_test.shape)\n",
    "\n",
    "x_test = x_test /255.0\n",
    "\n",
    "if os.path.isfile(finetune_weights):\n",
    "    model.load_weights(finetune_weights)\n",
    "\n",
    "p_test = model.predict(x_test, verbose = 1)\n",
    "\n",
    "preds = []\n",
    "for i in range(len(p_test)):\n",
    "    pos = np.argmax(p_test[i])\n",
    "    preds.append(list(label_map.keys())[list(label_map.values()).index(pos)])\n",
    "\n",
    "df_test['species'] = preds\n",
    "df_test.to_csv('/output/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
