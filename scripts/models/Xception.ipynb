{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os, cv2\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.applications import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:54<00:00,  4.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4750, 256, 256, 3)\n",
      "(4750, 12)\n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "label_map = {   \"Black-grass\"               :0,\n",
    "                \"Charlock\"                  :1,\n",
    "                \"Cleavers\"                  :2,\n",
    "                \"Common Chickweed\"          :3,\n",
    "                \"Common wheat\"              :4,\n",
    "                \"Fat Hen\"                   :5,\n",
    "                \"Loose Silky-bent\"          :6,\n",
    "                \"Maize\"                     :7,\n",
    "                \"Scentless Mayweed\"         :8,\n",
    "                \"Shepherds Purse\"           :9,\n",
    "                \"Small-flowered Cranesbill\" :10,\n",
    "                \"Sugar beet\"                :11}\n",
    "\n",
    "dim = 256\n",
    "\n",
    "# Preparing training data\n",
    "dirs = os.listdir(\"/input/plant_seedlings_classification/train/\")\n",
    "for k in tqdm(range(len(dirs))):    # Directory\n",
    "    files = os.listdir(\"/input/plant_seedlings_classification/train/{}\".format(dirs[k]))\n",
    "    for f in range(len(files)):     # Files\n",
    "        img = cv2.imread('/input/plant_seedlings_classification/train/{}/{}'.format(dirs[k], files[f]))\n",
    "        targets = np.zeros(12)\n",
    "        targets[label_map[dirs[k]]] = 1 \n",
    "        x_train.append(cv2.resize(img, (dim, dim)))\n",
    "        y_train.append(targets)\n",
    "    \n",
    "y_train = np.array(y_train, np.uint8)\n",
    "x_train = np.array(x_train, np.float32)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "#x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.01, random_state=42)\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.16, random_state=42) # Want a balanced split for all the classes\n",
    "for train_index, test_index in sss.split(x_train, y_train):\n",
    "    print(\"Using {} for training and {} for validation\".format(len(train_index), len(test_index)))\n",
    "    x_train, x_valid = x_train[train_index], x_train[test_index]\n",
    "    y_train, y_valid = y_train[train_index], y_train[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        rotation_range = 40,\n",
    "        horizontal_flip=True)\n",
    "                                  \n",
    "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "                                      \n",
    "weights = os.path.join('', '/output/weights.h5')\n",
    "\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "\n",
    "callbacks = [ EarlyStopping(monitor='val_loss', patience=5, verbose=0), \n",
    "              ModelCheckpoint(weights, monitor='val_loss', save_best_only=True, verbose=0),\n",
    "              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)]\n",
    "\n",
    "base_model = Xception(input_shape=(dim, dim, 3), include_top=False, weights='imagenet', pooling='avg') # Average pooling reduces output dimensions\n",
    "x = base_model.output\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(12, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "125/124 [==============================] - 249s - loss: 0.0211 - acc: 0.9927 - val_loss: 0.1085 - val_acc: 0.9671\n",
      "Epoch 2/20\n",
      "125/124 [==============================] - 249s - loss: 0.0256 - acc: 0.9932 - val_loss: 0.1003 - val_acc: 0.9737\n",
      "Epoch 3/20\n",
      "125/124 [==============================] - 249s - loss: 0.0217 - acc: 0.9934 - val_loss: 0.0902 - val_acc: 0.9671\n",
      "Epoch 4/20\n",
      "125/124 [==============================] - 249s - loss: 0.0148 - acc: 0.9950 - val_loss: 0.1177 - val_acc: 0.9697\n",
      "Epoch 5/20\n",
      "125/124 [==============================] - 249s - loss: 0.0203 - acc: 0.9955 - val_loss: 0.1193 - val_acc: 0.9724\n",
      "Epoch 6/20\n",
      "125/124 [==============================] - 249s - loss: 0.0127 - acc: 0.9975 - val_loss: 0.1061 - val_acc: 0.9737\n",
      "Epoch 7/20\n",
      "125/124 [==============================] - 249s - loss: 0.0138 - acc: 0.9945 - val_loss: 0.0943 - val_acc: 0.9789\n",
      "Epoch 8/20\n",
      "125/124 [==============================] - 248s - loss: 0.0127 - acc: 0.9969 - val_loss: 0.1148 - val_acc: 0.9697\n",
      "Epoch 9/20\n",
      "125/124 [==============================] - 248s - loss: 0.0100 - acc: 0.9970 - val_loss: 0.0781 - val_acc: 0.9803\n",
      "Epoch 10/20\n",
      "125/124 [==============================] - 248s - loss: 0.0111 - acc: 0.9975 - val_loss: 0.1413 - val_acc: 0.9645\n",
      "Epoch 11/20\n",
      "125/124 [==============================] - 248s - loss: 0.0112 - acc: 0.9972 - val_loss: 0.1065 - val_acc: 0.9737\n",
      "Epoch 12/20\n",
      "125/124 [==============================] - 249s - loss: 0.0092 - acc: 0.9970 - val_loss: 0.1142 - val_acc: 0.9697\n",
      "Epoch 13/20\n",
      "125/124 [==============================] - 249s - loss: 0.0082 - acc: 0.9982 - val_loss: 0.1071 - val_acc: 0.9697\n",
      "Epoch 14/20\n",
      "125/124 [==============================] - 249s - loss: 0.0071 - acc: 0.9980 - val_loss: 0.1018 - val_acc: 0.9684\n",
      "Epoch 15/20\n",
      "125/124 [==============================] - 249s - loss: 0.0112 - acc: 0.9957 - val_loss: 0.0987 - val_acc: 0.9724\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd05b415160>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=len(x_train)/batch_size, \n",
    "                    validation_data=val_datagen.flow(x_valid, y_valid, batch_size=batch_size), \n",
    "                    validation_steps=len(x_valid)/batch_size,\n",
    "                    callbacks=callbacks,\n",
    "                    epochs=20, \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 794/794 [00:16<00:00, 48.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(794, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "# ------ TESTING ------\n",
    "x_test = []\n",
    "df_test = pd.read_csv('/input/plant_seedlings_classification/sample_submission.csv')\n",
    "\n",
    "for f, species in tqdm(df_test.values, miniters=100):\n",
    "    img = cv2.imread('/input/plant_seedlings_classification/test/{}'.format(f))\n",
    "    x_test.append(cv2.resize(img, (dim, dim)))\n",
    "\n",
    "x_test = np.array(x_test, np.float32)\n",
    "print(x_test.shape)\n",
    "\n",
    "x_test = x_test /255.0\n",
    "\n",
    "if os.path.isfile(weights):\n",
    "    model.load_weights(weights)\n",
    "\n",
    "p_test = model.predict(x_test, verbose = 1)\n",
    "\n",
    "preds = []\n",
    "for i in range(len(p_test)):\n",
    "    pos = np.argmax(p_test[i])\n",
    "    preds.append(list(label_map.keys())[list(label_map.values()).index(pos)])\n",
    "\n",
    "\n",
    "df_test['species'] = preds\n",
    "df_test.to_csv('/output/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
